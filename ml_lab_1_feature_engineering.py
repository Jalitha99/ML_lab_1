# -*- coding: utf-8 -*-
"""ML Lab 1 Feature Engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VEdw3uu-d82X8Q1dHywHcvEoXHBKPWtU
"""

import pandas as pd
import numpy as np

#Constants
L1 = 'label_1'
L2 = 'label_2'
L3 = 'label_3'
L4 = 'label_4'
LABELS = [L1, L2, L3, L4]
AGE_LABEL = L2
FEATURES =  [f'feature_{i}' for i in range(1,257)]

train_df = pd.read_csv("/content/sample_data/train.csv")
valid_df = pd.read_csv("/content/sample_data/valid.csv")
test_df = pd.read_csv("/content/sample_data/test.csv")

train_df.head()
valid_df.head()
test_df.head()

train_df.info()
train_df[L1].value_counts()

from pandas.core.arrays.period import validate_dtype_freq
# Scaling the data
from sklearn.preprocessing import RobustScaler # RobustScaler

x_train = {}
x_valid = {}
x_test = {}
y_train = {}
y_valid = {}

for target_label in LABELS:
  tr_df = train_df[train_df['label_2'].notna()] if target_label == 'label_2' else train_df # remove NA values in label 2
  vl_df = valid_df[valid_df['label_2'].notna()] if target_label == 'label_2' else valid_df
  t_df = test_df
  scaler = RobustScaler()
  x_train[target_label] = pd.DataFrame(scaler.fit_transform(tr_df.drop(LABELS, axis=1)), columns = FEATURES)
  x_test[target_label] = pd.DataFrame(scaler.fit_transform(t_df.drop(LABELS, axis=1)), columns = FEATURES)
  y_train[target_label] = tr_df[target_label]
  x_valid[target_label] = pd.DataFrame(scaler.transform(vl_df.drop(LABELS, axis=1)), columns = FEATURES)
  y_valid[target_label] = vl_df[target_label]

"""**For Label 1**

Without Feature Engineering
"""

x_train[L1]

# Training the model to predict label_1
from sklearn import svm

clf = svm.SVC(kernel='linear')

clf.fit(x_train[L1], y_train[L1])

from sklearn import metrics

y_pred = clf.predict(x_valid[L1]) # Predicting the validation data set

# accuracy of the validation data set
print(metrics.confusion_matrix(y_valid[L1], y_pred))
print(metrics.accuracy_score(y_valid[L1], y_pred))
print(metrics.precision_score(y_valid[L1], y_pred, average='weighted'))
print(metrics.recall_score(y_valid[L1], y_pred, average='weighted'))

# predicting the label_1 of the test data set
y_pred_test = clf.predict(x_test[L1])

y_pred_test

"""With Feature Engineering"""

# With  selectKBest method
from sklearn.feature_selection import SelectKBest, f_classif

selector =  SelectKBest(f_classif, k=150) #  with only using 150 features
x_new_train = selector.fit_transform(x_train[L1], y_train[L1])
x_new_valid = selector.transform(x_valid[L1])
x_new_test = selector.transform(x_test[L1])
print('shape training', x_new_train.shape)
print('shape validation', x_new_valid.shape)
print('shape test', x_new_test.shape)

clf = svm.SVC(kernel='linear')

clf.fit(x_new_train, y_train[L1])

y_pred = clf.predict(selector.transform(x_valid[L1]))
print(metrics.confusion_matrix(y_valid[L1], y_pred))
print(metrics.accuracy_score(y_valid[L1], y_pred))
print(metrics.precision_score(y_valid[L1], y_pred, average='weighted'))
print(metrics.recall_score(y_valid[L1], y_pred, average='weighted'))

# adding PCA to the selectKBest results
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95, svd_solver='full')
pca.fit(x_new_train)
x_train_trf = pd.DataFrame(pca.transform(x_new_train)) # transforming data to fit into pca
x_valid_trf = pd.DataFrame(pca.transform(x_new_valid))
x_test_trf = pd.DataFrame(pca.transform(x_new_test))
print('Shape after PCA x_train : ', x_train_trf.shape)
print('Shape after PCA x_valid : ', x_valid_trf.shape)
print('Shape after PCA x_test : ', x_valid_trf.shape)

clf = svm.SVC(kernel='linear')

clf.fit(x_train_trf, y_train[L1])

y_pred = clf.predict(x_valid_trf)
print(metrics.confusion_matrix(y_valid[L1], y_pred))
print(metrics.accuracy_score(y_valid[L1], y_pred))
print(metrics.precision_score(y_valid[L1], y_pred, average='weighted'))
print(metrics.recall_score(y_valid[L1], y_pred, average='weighted'))

# predicting the test data
y_pred_test_fe = clf.predict(x_test_trf)
y_pred_test_fe

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test,
    'Predicted labels after feature engineering': y_pred_test_fe,
    'No. of new features': x_test_trf.shape[1]
})


for i in range(x_test_trf.shape[1]):
    output_df[f'New feature {i+1}'] = x_test_trf.iloc[:, i]
output_df
output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/190294K_label_1.csv',index=False)

"""**Label 2**

Without Feature Engineering
"""

x_train[L2]

# Training the model to predict label_2
from sklearn import svm

clf = svm.SVC(kernel='linear')

clf.fit(x_train[L2], y_train[L2])

from sklearn import metrics

y_pred_2 = clf.predict(x_valid[L2]) # Predicting the validation data set

# accuracy of the validation data set
print(metrics.confusion_matrix(y_valid[L2], y_pred_2))
print(metrics.accuracy_score(y_valid[L2], y_pred_2))
print(metrics.precision_score(y_valid[L2], y_pred_2, average='weighted'))
print(metrics.recall_score(y_valid[L2], y_pred_2, average='weighted'))

# predicting the label_3 of the test data set
y_pred_test = clf.predict(x_test[L2])

y_pred_test

"""With Feature Engineering"""

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsRegressor  # Import KNeighborsRegressor
from sklearn import metrics

selector = SelectKBest(f_classif, k=50)
x_train_new = selector.fit_transform(x_train[L2], y_train[L2])

# Applying PCA to selected features
pca = PCA(n_components=0.95, svd_solver='full')
x_train_pca = pca.fit_transform(x_train_new)
print("Shape after PCA: ", x_train_pca.shape)

# Creating a validation set with feature transformations
x_valid_new = selector.transform(x_valid[L2])
x_valid_pca = pca.transform(x_valid_new)

x_test_new = selector.transform(x_test[L2])
x_test_pca = pca.transform(x_test_new)

# Creating a KNN regressor
regressor = KNeighborsRegressor(n_neighbors=5)

# Fitting the KNN regressor
regressor.fit(x_train_pca, y_train[L2])

# Make predictions on the validation set
y_pred = regressor.predict(x_valid_pca)

# KNN regressor's performance
print("Mean Absolute Error:", metrics.mean_absolute_error(y_valid[L2], y_pred))
print("Mean Squared Error:", metrics.mean_squared_error(y_valid[L2], y_pred))
print("R-squared:", metrics.r2_score(y_valid[L2], y_pred))

# Make predictions on the test set
y_pred_test_fe = regressor.predict(x_test_pca)

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test,
    'Predicted labels after feature engineering': y_pred_test_fe,
    'No. of new features': x_test_pca.shape[1]
})

for i in range(x_test_pca.shape[1]):
    output_df[f'New feature {i+1}'] = x_test_pca[:, i]
output_df
output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/190294K_label_2.csv',index=False)

"""**Label 3**

Without Feature Engineering
"""

x_train[L3]

# Training the model to predict label_1
from sklearn import svm

clf = svm.SVC(kernel='linear')

clf.fit(x_train[L3], y_train[L3])

from sklearn import metrics

y_pred_3 = clf.predict(x_valid[L3]) # Predicting the validation data set

# accuracy of the validation data set
print(metrics.confusion_matrix(y_valid[L3], y_pred_3))
print(metrics.accuracy_score(y_valid[L3], y_pred_3))
print(metrics.precision_score(y_valid[L3], y_pred_3, average='weighted'))
print(metrics.recall_score(y_valid[L3], y_pred_3, average='weighted'))

# predicting the label_3 of the test data set
y_pred_test = clf.predict(x_test[L3])

y_pred_test

"""With Feature Engineering"""

# With  selectKBest method
from sklearn.feature_selection import SelectKBest, f_classif

selector =  SelectKBest(f_classif, k=10) #  with only using 10 features
x_new_train = selector.fit_transform(x_train[L1], y_train[L1])
x_new_valid = selector.transform(x_valid[L1])
x_new_test = selector.transform(x_test[L1])
print('shape training', x_new_train.shape)
print('shape validation', x_new_valid.shape)
print('shape test', x_new_test.shape)

clf = svm.SVC(kernel='linear')

clf.fit(x_new_train, y_train[L3])

y_pred_3 = clf.predict(selector.transform(x_valid[L3]))
print(metrics.confusion_matrix(y_valid[L3], y_pred_3))
print(metrics.accuracy_score(y_valid[L3], y_pred_3))
print(metrics.precision_score(y_valid[L3], y_pred_3, average='weighted'))
print(metrics.recall_score(y_valid[L3], y_pred_3, average='weighted'))

# adding PCA to the selectKBest results
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95, svd_solver='full')
pca.fit(x_new_train)
x_train_trf = pd.DataFrame(pca.transform(x_new_train)) # transforming data to fit into pca
x_valid_trf = pd.DataFrame(pca.transform(x_new_valid))
x_test_trf = pd.DataFrame(pca.transform(x_new_test))
print('Shape after PCA x_train : ', x_train_trf.shape)
print('Shape after PCA x_valid : ', x_valid_trf.shape)
print('Shape after PCA x_test : ', x_valid_trf.shape)

clf = svm.SVC(kernel='linear')

clf.fit(x_train_trf, y_train[L3])

y_pred = clf.predict(x_valid_trf)
print(metrics.confusion_matrix(y_valid[L3], y_pred))
print(metrics.accuracy_score(y_valid[L3], y_pred))
print(metrics.precision_score(y_valid[L3], y_pred, average='weighted'))
print(metrics.recall_score(y_valid[L3], y_pred, average='weighted'))

# predicting the test data
y_pred_test_fe = clf.predict(x_test_trf)
y_pred_test_fe

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test,
    'Predicted labels after feature engineering': y_pred_test_fe,
    'No. of new features': x_test_trf.shape[1]
})


for i in range(x_test_trf.shape[1]):
    output_df[f'New feature {i+1}'] = x_test_trf.iloc[:, i]
output_df
output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/190294K_label_3.csv',index=False)

"""**For Label 4**

Without Feature Engineering
"""

x_train[L4]

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train[L4], y_train[L4])

from sklearn import metrics

y_pred = knn.predict(x_valid[L4]) # Predicting the validation data set

# accuracy of the validation data set
print(metrics.confusion_matrix(y_valid[L4], y_pred))
print(metrics.accuracy_score(y_valid[L4], y_pred))
print(metrics.precision_score(y_valid[L4], y_pred, average='weighted'))
print(metrics.recall_score(y_valid[L4], y_pred, average='weighted'))

# predicting the label_4 of the test data set
y_pred_test = knn.predict(x_test[L4])

y_pred_test

"""With Feature Engineering"""

# With  selectKBest method
from sklearn.feature_selection import SelectKBest, f_classif

selector =  SelectKBest(f_classif, k=40) #  with only using 40 features
x_new_train = selector.fit_transform(x_train[L4], y_train[L4])
x_new_valid = selector.transform(x_valid[L4])
x_new_test = selector.transform(x_test[L4])
print('shape training', x_new_train.shape)
print('shape validation', x_new_valid.shape)
print('shape test', x_new_test.shape)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_new_train, y_train[L4])

y_pred = knn.predict(selector.transform(x_valid[L4]))
print(metrics.confusion_matrix(y_valid[L4], y_pred))
print(metrics.accuracy_score(y_valid[L4], y_pred))
print(metrics.precision_score(y_valid[L4], y_pred, average='weighted'))
print(metrics.recall_score(y_valid[L4], y_pred, average='weighted'))

# adding PCA to the selectKBest results
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95, svd_solver='full')
pca.fit(x_new_train)
x_train_trf = pd.DataFrame(pca.transform(x_new_train)) # transforming data to fit into pca
x_valid_trf = pd.DataFrame(pca.transform(x_new_valid))
x_test_trf = pd.DataFrame(pca.transform(x_new_test))
print('Shape after PCA x_train : ', x_train_trf.shape)
print('Shape after PCA x_valid : ', x_valid_trf.shape)
print('Shape after PCA x_test : ', x_valid_trf.shape)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train_trf, y_train[L4])

y_pred = knn.predict(x_valid_trf)
print(metrics.confusion_matrix(y_valid[L4], y_pred))
print(metrics.accuracy_score(y_valid[L4], y_pred))
print(metrics.precision_score(y_valid[L4], y_pred, average='weighted'))
print(metrics.recall_score(y_valid[L4], y_pred, average='weighted'))

# predicting the test data
y_pred_test_fe = knn.predict(x_test_trf)
y_pred_test_fe

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test,
    'Predicted labels after feature engineering': y_pred_test_fe,
    'No. of new features': x_test_trf.shape[1]
})


for i in range(x_test_trf.shape[1]):
    output_df[f'New feature {i+1}'] = x_test_trf.iloc[:, i]
output_df
output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/190294K_label_4.csv',index=False)